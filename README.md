<div align="center">

# âš¡ AWS Realtime Medallion ML Feature Platform

[![AWS](https://img.shields.io/badge/AWS-%23FF9900.svg?style=for-the-badge&logo=amazon-aws&logoColor=white)](https://aws.amazon.com/)
[![Terraform](https://img.shields.io/badge/terraform-%235835CC.svg?style=for-the-badge&logo=terraform&logoColor=white)](https://www.terraform.io/)
[![Apache Spark](https://img.shields.io/badge/Apache%20Spark-E25A1C?style=for-the-badge&logo=apachespark&logoColor=white)](https://spark.apache.org/)
[![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)](https://www.python.org/)

**Enterprise-grade serverless data pipeline for real-time ML feature engineering**

[ğŸ“– Documentation](#documentation) â€¢ [ğŸš€ Quick Start](#deployment-guide) â€¢ [ğŸ§ª Testing](#-end-to-end-testing-guide) â€¢ [ğŸ’° Cost](#cost-estimation)

</div>

---

## ğŸ¯ Overview

A production-ready **Serverless Medallion Architecture** built on AWS for processing real-time card transaction data, performing advanced feature engineering, and generating ML-ready datasets.

### âœ¨ Key Features

<table>
<tr>
<td width="50%">

ğŸŒ **Region**  
`ap-southeast-1` (Singapore)

ğŸ“Š **Data Architecture**  
Bronze â†’ Silver â†’ Gold (Medallion)

âš¡ **Real-time Processing**  
Kinesis + Firehose + EMR Serverless

</td>
<td width="50%">

ğŸ¯ **Feature Platform**  
SageMaker Feature Store (Online + Offline)

ğŸ“¦ **Batch Processing**  
Daily training & inference datasets

ğŸ”„ **Orchestration**  
Step Functions state machine

</td>
</tr>
<tr>
<td colspan="2">

ğŸ—ï¸ **Infrastructure as Code**: Terraform modules  
ğŸ” **CI/CD**: GitHub Actions with OIDC

</td>
</tr>
</table>

---

## ğŸ—ï¸ Architecture Diagram

<div align="center">

```mermaid
graph TB
    subgraph "Ingestion Layer"
        A[Kinesis Stream] -->|Stream| B[Kinesis Firehose]
    end
    
    subgraph "Storage - Bronze Layer"
        B -->|JSON.gz| C[S3 Bronze]
    end
    
    subgraph "Orchestration"
        D[EventBridge Rules]
        D -->|Every 10min| E[Step Functions]
        D -->|Daily| E
    end
    
    subgraph "Processing Layer"
        E -->|Stream Mode| F[EMR Serverless<br/>silver_and_gold.py]
        E -->|Daily Mode| G[EMR Serverless<br/>build_datasets.py]
    end
    
    subgraph "Storage - Curated Layers"
        F -->|Parquet| H[S3 Silver]
        F -->|Parquet| I[S3 Gold]
        G -->|Parquet| J[S3 Training/Inference]
    end
    
    subgraph "Feature Platform"
        I -->|Upsert| K[SageMaker Feature Store]
        K -->|Online Store| L[DynamoDB]
        K -->|Offline Store| M[S3]
    end
    
    C -.->|Trigger| D
    
    style A fill:#FF9900
    style B fill:#FF9900
    style F fill:#E25A1C
    style G fill:#E25A1C
    style K fill:#FF9900,stroke:#232F3E,stroke-width:3px,color:#000
    style E fill:#D86613
```

</div>

---

## ğŸ“‚ Project Structure

```
ğŸ“¦ AWS-Batch-Realtime-Medallion-ML-Feature-Platform-Pipeline
â”œâ”€â”€ ğŸ—ï¸  infra/terraform/                # Infrastructure as Code
â”‚   â”œâ”€â”€ ğŸ“„ main.tf                      # Root module
â”‚   â”œâ”€â”€ ğŸ”§ variables.tf                 # Variable definitions
â”‚   â”œâ”€â”€ ğŸ“Š outputs.tf                   # Output values
â”‚   â””â”€â”€ ğŸ“¦ modules/                     # Reusable Terraform modules
â”‚       â”œâ”€â”€ ğŸª£  s3_datalake/            # S3 bucket configuration
â”‚       â”œâ”€â”€ ğŸŒŠ kinesis_firehose/        # Kinesis streaming setup
â”‚       â”œâ”€â”€ âš¡ emr_serverless/          # EMR Serverless config
â”‚       â”œâ”€â”€ ğŸ—„ï¸  glue/                   # Glue catalog
â”‚       â”œâ”€â”€ ğŸ¯ sagemaker_featurestore/  # Feature Store setup
â”‚       â”œâ”€â”€ ğŸ”„ step_functions/          # Workflow orchestration
â”‚       â”œâ”€â”€ â° eventbridge/             # Event scheduling
â”‚       â””â”€â”€ ğŸ“ˆ cloudwatch/              # Monitoring & alarms
â”œâ”€â”€ ğŸ­ state_machines/                  # Step Functions definitions
â”‚   â””â”€â”€ stream_pipeline.asl.json
â”œâ”€â”€ âš™ï¸  spark_jobs/                     # PySpark processing jobs
â”‚   â”œâ”€â”€ silver_and_gold.py              # Bronze â†’ Silver â†’ Gold
â”‚   â””â”€â”€ build_datasets.py               # Training/inference datasets
â”œâ”€â”€ ğŸ¯ feature_store/                   # Feature Store utilities
â”‚   â”œâ”€â”€ register_feature_groups.py
â”‚   â””â”€â”€ ingest_features.py
â”œâ”€â”€ ğŸ”§ scripts/                         # Utility scripts
â”‚   â””â”€â”€ transform_and_prepare_sample_data.py
â”œâ”€â”€ ğŸ“Š sample_data/                     # Sample transaction data
â”‚   â””â”€â”€ bronze_sample_transactions.json
â”œâ”€â”€ ğŸ” .github/workflows/               # CI/CD pipelines
â”‚   â”œâ”€â”€ deploy.yml                      # Deployment workflow
â”‚   â””â”€â”€ destroy.yml                     # Teardown workflow
â”œâ”€â”€ ğŸ“– DEPLOYMENT_GUIDE.md              # Detailed deployment guide
â”œâ”€â”€ ğŸ§ª E2E_TESTING_GUIDE.md             # End-to-end testing guide
â”œâ”€â”€ ğŸ“‹ requirements.txt                 # Python dependencies
â””â”€â”€ ğŸ“ README.md                        # This file
```

---

## ğŸ“Š Data Model

### ğŸ¥‰ Bronze Layer
**Raw streaming data ingestion**

```yaml
Path: s3://bucket/bronze/streaming/card_authorization/ingest_dt=YYYY/MM/DD/HH/mm/*.json.gz
Format: Compressed NDJSON
Schema:
  - event_id: string       # Unique transaction identifier
  - card_id: string        # Card identifier
  - ts: timestamp          # Transaction timestamp (Unix)
  - merchant_id: string    # Merchant identifier
  - amount: float          # Transaction amount
  - currency: string       # Currency code (USD, EUR, etc.)
  - country: string        # Country code
  - pos_mode: string       # POS mode (chip, contactless, etc.)
```

### ğŸ¥ˆ Silver Layer
**Cleaned & validated data**

```yaml
Path: s3://bucket/silver/card_transactions/dt=YYYY-MM-DD/*.parquet
Format: Parquet (Snappy)
Processing: Deduplication, validation, type casting
Partitioning: Daily (dt=YYYY-MM-DD)
```

### ğŸ¥‡ Gold Layer
**Feature-enriched data**

```yaml
Path: s3://bucket/gold/card_features/dt=YYYY-MM-DD/*.parquet
Format: Parquet (Snappy)
Features:
  ğŸ“ˆ txn_count_1h: int          # Transactions in last 1 hour
  ğŸ’° txn_amount_1h: float       # Total amount in last 1 hour
  ğŸª merchant_count_24h: int    # Unique merchants in 24 hours
  ğŸ“Š avg_amount_7d: float       # 7-day average transaction amount
```

### ğŸ“ Training/Inference Datasets

| Dataset | Path | Purpose |
|---------|------|---------|
| **Training** | `s3://bucket/gold/training/dt=YYYY-MM-DD/train/*.parquet` | Model training |
| **Validation** | `s3://bucket/gold/training/dt=YYYY-MM-DD/validation/*.parquet` | Model validation |
| **Inference** | `s3://bucket/gold/inference/dt=YYYY-MM-DD/*.parquet` | Real-time predictions |

---

## ğŸš€ Deployment Guide

### Prerequisites

<table>
<tr>
<td>

**â˜ï¸ AWS Account**  
Appropriate IAM permissions for resource creation

</td>
<td>

**ğŸ”§ AWS CLI**  
Configured with credentials (`aws configure`)

</td>
<td>

**ğŸ™ GitHub Repository**  
OIDC configured for GitHub Actions

</td>
</tr>
</table>

> ğŸ“– **Detailed deployment steps**: See [`DEPLOYMENT_GUIDE.md`](DEPLOYMENT_GUIDE.md) for comprehensive instructions including Terraform Backend setup, GitHub OIDC configuration, and automated CI/CD deployment.

---

## ğŸ§ª End-to-End Testing Guide

> ğŸ”¬ **Complete testing procedures**: Refer to [`E2E_TESTING_GUIDE.md`](E2E_TESTING_GUIDE.md) for:
> - ğŸ“¤ Sample data preparation
> - âš¡ Pipeline triggering (stream & batch modes)
> - ğŸ‘€ Monitoring & observability
> - âœ… Result verification

---

## ğŸ“ˆ Monitoring

### CloudWatch Dashboard

<div align="center">

**Access Path**: AWS Console â†’ CloudWatch â†’ Dashboards â†’ `{project}-{env}-dashboard`

| Metric | Description |
|--------|-------------|
| ğŸ”„ **Step Functions Status** | Execution success/failure rates |
| âš¡ **EMR Job Success Rate** | Spark job completion metrics |
| ğŸŒŠ **Firehose Delivery Rate** | Stream delivery performance |
| ğŸ“Š **Custom Pipeline Metrics** | End-to-end processing latency |

</div>

### âš ï¸ Alarms

| Alarm | Trigger Condition | Action |
|-------|-------------------|--------|
| ğŸš¨ **SFN Execution Failed** | Step Functions execution failure | SNS notification |
| ğŸš¨ **Firehose Delivery Failed** | Success rate < 95% | SNS notification |

---

## ğŸ’° Cost Estimation

<div align="center">

**Development Environment** â€¢ ~1M events/day

| Service | Usage | Monthly Cost |
|:--------|:------|:------------:|
| ğŸŒŠ Kinesis Data Stream | 1 shard, 24/7 | **$11** |
| ğŸ”¥ Kinesis Firehose | 1M records | **$5** |
| ğŸª£ S3 Storage | 100 GB | **$2.3** |
| âš¡ EMR Serverless | 144 runs/day, 5min each | **$20** |
| ğŸ”„ Step Functions | 144 executions/day | **$0.03** |
| ğŸ¯ SageMaker Feature Store | 1M writes, 100K reads | **$10** |
| | **Total Estimate** | **~$50/month** |

</div>

### ğŸ’¡ Cost Optimization Tips

```yaml
âœ… S3 Lifecycle Policies: 30-day retention â†’ Save on storage
âœ… EMR Auto-stop: 15min idle timeout â†’ Pay only for active jobs  
âœ… Kinesis On-Demand: Variable workloads â†’ No over-provisioning
âœ… EventBridge Disable: Manual triggering during dev â†’ Near-zero fixed costs
```

---

## ğŸ”§ Troubleshooting

<details>
<summary><b>âš¡ EMR Job Failure</b></summary>

1. **Check CloudWatch Logs**
   ```bash
   aws logs tail /aws/emr-serverless/applications/{app-id}/jobs/{job-id} --follow
   ```

2. **Verify S3 Permissions**
   - Ensure EMR job role has `s3:GetObject`, `s3:PutObject` permissions
   - Check bucket policies and ACLs

3. **Validate Spark Code**
   - Test locally with `spark-submit --master local[*]`
   - Check for Python syntax errors or missing dependencies

</details>

<details>
<summary><b>ğŸ¯ Feature Store Upsert Failure</b></summary>

1. **Check Feature Group Status**
   ```bash
   aws sagemaker describe-feature-group --feature-group-name rt_card_features_v1
   ```
   Status should be `Created`

2. **Verify IAM Permissions**
   - Role needs: `sagemaker-featurestore-runtime:BatchPutRecord`
   - Check trust relationship for SageMaker service

3. **Validate Schema Consistency**
   - Ensure feature definitions match data schema
   - Check data types (String, Integral, Fractional)

</details>

<details>
<summary><b>ğŸ”„ Step Functions Timeout</b></summary>

1. **Increase EMR Capacity**
   ```hcl
   # In terraform.tfvars
   emr_driver_cores = 2
   emr_executor_cores = 2
   ```

2. **Adjust Wait States**
   ```json
   {
     "Type": "Wait",
     "Seconds": 300  // Increase if needed
   }
   ```

3. **Check Job Bottlenecks**
   - Review Spark UI for stage-level metrics
   - Identify data skew or shuffle operations

</details>

---

##  Documentation

| Document | Description |
|----------|-------------|
| ğŸ“– [DEPLOYMENT_GUIDE.md](DEPLOYMENT_GUIDE.md) | Step-by-step deployment instructions |
| ğŸ§ª [E2E_TESTING_GUIDE.md](E2E_TESTING_GUIDE.md) | End-to-end testing procedures |
| ğŸ”— [AWS EMR Serverless](https://docs.aws.amazon.com/emr/latest/EMR-Serverless-UserGuide/) | Official EMR documentation |
| ğŸ”— [SageMaker Feature Store](https://docs.aws.amazon.com/sagemaker/latest/dg/feature-store.html) | Feature Store guide |
| ğŸ”— [AWS Step Functions](https://docs.aws.amazon.com/step-functions/) | Workflow orchestration |

---

<div align="center">

## ğŸ“„ License

**MIT License** - Patrick Cheung

---

**Author**: Patrick Cheung | **Last Updated**: October 24, 2025

â­ **Star this repo** if you find it useful!

</div>
